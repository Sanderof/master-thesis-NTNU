\chapter{Related work}\label{chap:relatedwork}
% Include concrete papers that have developed some system similar to ours
% Talk about the data used, and why we collected our own

\begin{comment}
    1. What we base our work upon ✅
    2. Use of response systems ✅
    3. Sentiment analysis ✅
    4. Topic modelling + sentiment analysis ✅
    5. SETSUM -> More comprehensive ✅
    6. Assessment and feedback through LLM ✅
\end{comment}

In the papers \cite{stoica2016} and \cite{stoica2017}, Stoica - our co-supervisor - and colleagues present the iLike response system. This system lets the audience - in most cases students - use their own network-connected devices to respond to questions. The goal of the papers was to visualise the open-text responses in real-time in order to use the information in a dialogue with the audience. iLike employs two forms of visualisation of open-text responses. Firstly, a list of all the responses sorted by frequency, which may be useful when the responses are short and many of them are expected to be the same. Secondly, a word cloud showing the most freqeuntly used words. The papers put the most emphasis on the word cloud and highlight a range of challenges with visualising the responses. These challenges include the presence of typos and irregular use of diatcritics in responses, profanities, the repeating of words - referred to as ``shouting'' - common words that are not meaningful - usually referred to as stop-words - and words with similar meaning. The second paper \cite{stoica2017} attempts to deal with some of these challenges through extending the pre-processing pipeline with several filters. It also adds the functionality of drilling into the individual words to see the context within which they originally appear. As the future work outlined by this last paper, in addition to addressing the other identified challenges, incorporating more elements of text mining and natural language processing is mentioned for extracting more meaning from the open-text responses and improve the visualisations. Our work on this master thesis, is part of this future work in incorporating text mining techniques to help instructors make sense of open-text responses in real-time to foster dialogue and engagement in the classroom.

While searching for research on the use of text mining with response systems, we have found a minimal amount of studies. It is, therefore, clear that more research is needed within this domain. One study, however, that addresses this domain is one conducted by Tseng et al. \cite{tseng2018}. They have in their study used topic modelling on open-text responses collected through an instant response system (IRS). They tested three different clustering methods, namely Hierarchical Agglomerative Clustering (HAC), Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Additionally, the developed a processing pipeline which makes use of various rule sets, lexicons and text corpora to deal with punctuation, synonyms, translation when multiple languges are present and for preparing the text for use with the different clustering methods. The paper cannot determine whether any of the three methods are better than the others, and they underline that the presence of many very short text responses hamper the methods' performance and that the choice of which features to extract heavily influences the methods. The paper is only a preliminary exploration of topic modelling used with response systems, so the results are limited. Despite this, the module developed in the study has been used by some instructors in Thailand and Taiwan with the LSA clustering method, where it has proved useful for the instructors. This implies that further research on the use of topic modelling will be valuable. It also provides some evidence that such a system can handle multi-lingual responses. In our case, we will have to handle both English, Norwegian and a mix of both.

Despite limited application of text mining in combination with response systems in the literature, text mining has been applied extensively in the education domain. Neumann et al. \cite{neumann2021} asked in their paper whether sentiment analysis can efficiently measure student emotions from textual feedback and whether the emotions provide instructors with valuable insight. The feedback was collected from students as short unit-of-study reviews related to assignments accompanied by five-star ratings. The paper uses VADER, an unsupervised, rule-based sentiment analysis method. The method consults a lexicon with English terms annotated with sentiment scores to calculate a total sentiment score for a piece of text. They conclude that sentiment analysis provides a more accurate representation of students' emotions than do star ratings - a type of likert-scale response - and that sentiment analysis is a viable alternative to manual reading and labelling of textual reviews, this latter being significantly more time-consuming. The quickness of the feedback also makes it easier for instructors to monitor changes in emotions over time and simple tools, such as word-clouds, can be used to gain insights into why students had a negative or positive feeling about a particular assignment. While the concept of sentiment analysis seems promising, Neumann et al. underlines that the VADER lexicon would require tuning to better match the use-context and that other sentiment analysis methods should be explored.

To gain deeper insight into students' open-text responses, it is also possible to combine different text mining methods. This is what Rääf et al. \cite{raaf2021} have done when analysing learners' reviews from an MOOC. In their study, they start by scraping reviews from the MOOC, cleaning and preprocessing them, and then they combine topic modelling, using LDA, and sentiment analysis, using VADER to assign a sentiment score to each identified topic. This approach makes for more fine-grained sentiment analysis, helping instructors understand what aspects of their courses trigger which emotions in the learners. The study successfully applied the approach to over 30000 MOOC reviews to identify trends and feelings towards five different courses during covid-19. The sample size and context of this study is, however, quite different from what our system would be subjected to, but it shows that combining different text mining can give more fine-grained and meaningful insight. A factor we must keep in mind from this study is that labelling the different topics produced by LDA is done manually, which would not work well in a real-time classroom setting. Some method for automatic labelling must, therefore, be included in our case. The study also outlines that VADER has some difficulty with distinguishing negative texts from neutral ones, most likely due to negation words.

Another example of combining various text mining methods, is the SETSUM system developed by Hu et al. \cite{setsum2022}. The SETSUM system is proposed to provide instructors and higher education management with more in-depth and intuitive reports from Student Evaluations of Teaching (SETs). The analysis of the open-text responses in the SETs is performed by applying sentiment analysis, aspect extraction and extractive summarisation. The system also visualises the results from applying these methods in the final reports using a set of different interactive charts. Differently from the other studies we have looked at, Hu et al. employs machine-learning for the sentiment analysis. This is done in the form of transformer models, more specifically a fine-tuned version of the RoBERTa model. The aspect extraction approach also differs from the topic modelling in the other studies by being weakly supervised. The model is fed with predefined, labelled aspects or topics, each with a set of seed words used to guide the model in assigning open-text responses to the different aspects. This uses a model called Multi Seed Aspect Extractor (MATE). The extractive summarisation method is developed to a greater extent by the researchers themselves. It uses another transformer model, called Sentence-BERT, to create embeddings for each sentence. Then cosine similarity is used to rank the sentences for each aspect. A sentence is then selected by a greedy unsupervised sentence extraction algorithm to take the place as the summary for the given aspect. The system is somewhat complex, but the reports it produces help instructors interpret SET results more efficiently and oftentimes with less bias, according to a survey conducted by the researchers. This study tells us that good visualisations are key, and that combining text mining methods provide more ways for instructors to visualise and udnerstand the information from students' open-text responses. It also goes to show that machine-learning models are viable instruments for open-text analysis in education. However, picking the aspects and seed-words to use requires the context in which the responses are collected to be rather static. This will not be the case for us, as the context of a question asked through a response system will vary dramatically from lecture to lecture and from what insights the instructor wants, such as guaging students' understanding, getting feedback or prompting reflection. Fine-tuning transformer models will also require a lot of data, and how generalisable these models will be is hard to say.

From the public release of ChatGPT in March 2023, the popularity of transformer models has exploded. We have seen in \cite{setsum2022} that finetuned BERT models can be used to interpret open-text responses, but also other language models may be used for this purpose, both through fine-tuning and prompting. Carpenter et al. \cite{carpenter2024} have tested and compared the four large language models, Llama 2, GPT-3.5, GPT-4 and FLAN-T5 in their ability to assess students' explanations to course concepts. The explanations were collected through a response system. The models' prompts were given the instruction of assessing the students' explanations and the question asked. Additionally, the researchers experimneted with including a question-specific assessment rubric, an examplar response created by the teacher, ten labelled student responses to the same question, and combinations of these pieces of information. Due to FLAN-T5's small model size, it was also fine-tuned for each version of the prompt. The task given to the models can be categorised as a multi-class classification task, where they need to label each explanation as correct, partially correct or incorrect. The study found that the fine-tuned FLAN-T5 with the assessment rubric and an examplar response in the prompt performed the best, together with GPT-4 with ten student responses in the prompt. We should keep in mind here that FLAN-T5 is a small-sized open-source model, while GPT-4 is a huge propriatary model that needs to be accessed through OpenAI's API for a per-token fee. Both of these methods require some examples of student responses to the asked questions for them to perform well, FLAN-T5 requires more than GPT-4. This is not necessarily something that is available, especially for ad-hoc questions. Creating the assessment rubrics and examplar responses also adds overhead that may not be feasible in a real-time lecture setting. The paper itself finds that GPT-4 with only the assessment rubric added to the prompt may be the most viable solution for use with a real-time response system.

We see that there are many ways in which text mining methods can be applied to analyse open-text responses in education. Methods can either be used in isolation or several methods can be combined to gain a richer analysis. Both more traditional text mining approaches, such as topic modelling with LDA, and newer approaches using large language models are viable methods, and how we visualise the analysis results is of utmost importance for the usefulness in the eyes of the instructors. While there has been performed little research on text mining in a real-time education setting using response systems, we can undoubtably gain a lot of knowledge and inspiration from literature applying text mining in other education contexts. However, the real-time setting on which we are focusing sets a lot of requirements for our system. We may not have enough data to train or tune machine learning models, there may not be time for the instructor to perform any manual labelling of topics, and the processing must be finished within the span of a few minutes maximum.



