\chapter{Discussion}\label{sec:discussion}

\begin{comment}
The results chapter should simply present the results of applying the methods presented in the method chapter without further ado. This chapter will typically contain many graphs, tables, etc. Sometimes it is natural to discuss the results as they are presented, combining them into a `Results and Discussion' chapter, but more often they are kept separate.

Here you should discuss all aspect of your thesis and project. How did the process work? Which choices did you make, and what did you learn from it? What were the pros and cons? What would you have done differently if you were to undertake the same project over again, both in terms of process and product? What are the societal consequences of your work?

IT2810 - Web development
IT3021 - Game+
TET4205 - Power system analysis 2
IT3402 - Service design
Trond Morten

Innsida - Two reached out, one provided data
Excited - No response
Contact persons - 3
Co-supervisor contact - 1
Backup - Peer reviews

Do we have enough data?
\end{comment}

\section{Collected data}
% Write about what we know about the data. In the final thesis, have a more in-depth analysis

As described in section \ref{sec:data-collection-strategy}, we have reached out to potential collaborators through a few different channels with the goal of collected open-text responses submitted by students through a response system.

Firstly, our attempt at reaching out to the members of Excited did not yield any result. As for the note published on Innsida, we had two lecturers reach out to us. One of them, a lecturer of an electrical engineering course, has collected open-text responses through Mentimeter this semester at regular intervals and shared the responses with us. The obtained dataset contains various types of questions, such as multiple-choice and scales, but also open-text questions. There are in total 337 responses, as of September 19th, written in English. The length varies from a single word to a couple of short sentences. The amount may increase if we receive the responses from the second half of the semester.

In terms of lecturers we have reached out to directly, we have received responses from three, all within the domain of computer science. Two of the datasets are quite small. The first of them consists of 24 Norwegian responses distributed over three questions. The second consists of eight responses in Norwegian also distributed over three questions. Both datasets are collected through Mentimeter. These two datasets are hardly large enough to be of much use, as it is easy for a lecturer to look through all the responses to a question during a lecturer; there is no need for any computer-assisted analysis of the responses. The third dataset, on the other hand, is more extensive. It contains 15 questions with 478 English responses with between 19 and 43 responses per question. The responses vary from a single word to a short sentence. An interesting feature of this dataset is that some responses contain emojis, which is something that our system must handle.

The last collected dataset was shared with us by a contact of our co-supervisor. They are also a lecturer at NTNU, and the dataset contains around 1000 responses collected over a few years in engineering courses. The responses are mainly in Norwegian. We are currently not in possession of the dataset, but we will receive it before starting experimentations next semester.

In conclusion, we have managed to collect three datasets of sufficient size - around 1800 responses - and a sufficient number of responses per question for them to be valuable during experimentation with AI methods. We also have both English and Norwegian responses. At this point in time, we have not yet analysed the datasets. In the final master thesis, we will include a thorough analysis of the collected datasets.

\section{Systematic literature review}\label{sec:results-slr}
% Talk about the plans for the SLR for the next semester
% Mention resources, search string, number of results, exclusions
In this section, we will present the results we have thus far from our SLR. We will also explain what is missing at this point in time and lay out the road ahead for the upcoming semester, spring 2025.

\subsection{Study selection}
% Describe the results of the search and selection process, from the number of records identified in the search to the number of studies included in the review, ideally using a flow diagram
Following the selection process outlined in section \ref{sec:slr-selection-process}, we obtained the results as shown in the PRISMA flowchart of figure \ref{fig:prisma-flowchart}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/PRISMA diagram.png}
    \caption{PRISMA flowchart depicting the results of the study selection process.}
    \label{fig:prisma-flowchart}
\end{figure}

When we entered our search query into the five scientific databases and applied the respective filters and limits, we ended up with a total of 2862 papers. 373 of these came from Web of science, 887 from Scopus, only seven from ACM Digital library, 302 from Proquest and 1293 from ERIC. After having exported all the papers to Endnote, we noticed that 696 of the papers coming from ERIC were marked as reports instead of journal articles, despite having set the \textit{Journal articles} filter. These were removed, as we are interested in paper describing new research. After that we used Endnote's built-in functionality to remove duplicates, which resulted in 226 fewer papers. Lastly, two papers were removed that were not in English, leaving us with 1938 papers for screening.

When screening the titles and abstracts, we applied the eligibility criteria. 1883 papers were excluded after this, leaving 55 papers. Five of these did not have full text available. 50 papers were subsequently full text screened and the exclusion reason reported. For the IDs of the eligibility criteria refer to table \ref{tab:eligibilitycriteria}.

We, thereafter, performed an iteration of forwards and backwards snowballing on the 25 papers that were included after full text screening. 15 papers were found through forwards snowballing, which is to say through the papers' citations, and 32 through backwards snowballing, which is to say through the papers' references. Some papers appeared both in citations and references, in which case they are reported as coming from backwards snowballing. Papers that were already found through searching the databases were immediatly excluded as duplicates and not part of the reported statistics in figure \ref{fig:prisma-flowchart}. Snowballing resulted in 47 new papers, of which 14 were excluded due to them not fullfilling inclusion criterion IC7. We were then left with 33 papers from snowballing, with the total count of included papers being 58. These 58 went on to be part of the data extraction process.

While these 58 papers are all published in the period 2015-2024, we have compiled the bar chart in figure \ref{fig:papersbyyear} to get an idea of the papers' timeline. Most of the papers were published in 2019, 2023 and 2020. However, the chart shows no significant trend.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/papers-by-year.png}
    \caption{Distribution of included papers by publication year}
    \label{fig:papersbyyear}
\end{figure}

\subsection{Data extraction}
Despite the data extraction not being fully completed, we can comment on what we have extracted so far. Of the 58 included papers, ten have still not had data extracted. We have not yet performed any analysis on the data. The reporting in our Excel document regarding the codes, does not yet lend itself to easy data analysis due to formatting, misalignments in the use of codes and their naming, and some blank rows and cells. We find that it is better to perform a proper analysis when the data extraction process is at a more mature stage.

A first observation we have made is the lack of studies using response systems for data collection. This is in line with the impression we had in the early phases when searching scientific databases in a less structured way for literature and as we have commented upon in chapter \ref{chap:relatedwork}. From the 48 papers with data extracted, only a single paper has been coded with ``Response system'', and this study only uses response systems as one of two systems for collecting open-text responses, the other being feedback comments from the MOOC Coursera. On the other hand, there are 15 papers marked as having a ``Synchronous'' learning environment, i.e. that the data is collected in a classroom or lecture setting. In all cases the open-text analysis i performed with AI tools by the researchers at a later point, but the data in these 15 studies can still be considered more representative of a real-time response system scenario than data collected from surveys, MOOC comments or reviews asynchronously. Though the data collection is synchronous, what system is used differs. Five papers simply mention that the data was collected during class or a lecture without saying how. Three used online surveys, such as Google form, two used a learning management system in some way, and four used some form of mobile app, and one used a response system. One can argue that the learning management systems and mobile apps act as response systems, in which case we have seven studies investigating open-text responses collected through response systems. This is why we need to further refine and discuss the codes used, to simplify the data analysis and agree on what is considered a response system and not.

A second observation we can make is which AI methods are used. So far we have identified four AI tasks and a good amount of AI algorithms corresponding to these tasks. Keep in mind that the number of papers with each code is subject to change as we progress in the data extraction process.
\todo{Maybe sources for the different AI tasks?}
\begin{itemize}
    \item \textbf{Topic modelling:}
    19 of the papers have been coded with ``Topic modelling''. Topic modelling is the act of grouping text documents - such as open-text responses or single sentences - into a given number of themes based on semantic similarity. We have decided to include both unsupervised, semi-supervised and supervised AI methods in our definition. Some of the algorithms identified for the use of topic modelling are Latent Dirichlet Allocation (LDA), K-means and K-Medoids. LDA has been used by several studies both in its unsupervised form and in a semi-supervised form where seed words are used to guide the discovery of themes.
    \item \textbf{Sentiment analysis:}
    27 papers are coded ``Sentiment analysis'', which appears to be the most used approach to analysing open-text responses in education. Sentiment analysis includes any AI method for extracting feelings, emotions or sentiment scores from text. Both lexicon-based and machine learning-based algorithms have been used. Lexicon-based sentiment analysis calculates scores for how negative, neutral or positive some piece of text is using a lexicon of words annotated with per-word sentiment scores. Some lexicons that are used are the MPQA corpus, SentiStrengthId, NRC emotion lexicon, Afinn lexicon and the VADER lexicon. VADER has been the most prominent one among the included papers. For machine learning, some used methods are Decision trees, Naive Bayes, Support Vector Machine, K-Nearest Neighbour, Random Forest, and various BERT-based models. Many of these are more general text classification models, but since sentiment analysis is employed to such a great extent, we have decided to set it as its own AI task instead of referring to it as simply text classification.
    \item \textbf{Text classification:}
    10 papers have the code ``Text classification''. Text classification is the act of assigning pieces of text to pre-defined classes or categories. Classification of sentiments is considered as sentiment analysis instead of text classification. Some employed AI methods in this regard are different rule-based classifiers, conditional inference trees, logistic regressions, Markov Random Field, as well as some of the ones used for sentiment analysis.
    \item \textbf{Text summarisation:}
    4 papers deal with ``summarisation''. Summarisation is a boarder task than the three others and may use classification, topic modelling or sentiment analysis as part of the process. The task concerns itself with providing a textual summary over all input text. This can either be abstract, where new text is generated, or extractive, where pieces of the input text are extracted and put together into a summary. These papers use an array of the aforementioned AI methods in addition to methods such as LexRank for ranking pieces of text according to relevance.
\end{itemize}

Another relevant observation is the proposed future work reported by the included papers. What appears to be the the most commonly mentioned one, is to increase generalisability. This means that the AI methods in the study should be tested on more diverse data, for instance, on different courses, with different natural languages or a larger sample size. Somewhat related to this, a few studies reported that the evaluation process needs to be improved to be more certain of how well the AI methods perform. Another rather common mention was to increase granularity. Several studies found that performing, for instance, sentiment analysis over all open-text responses did not give enough insight. As such, performing sentiment analysis over each topic could give more insight into why the students feel a certain way. To help with insight, some studies also saw the need for developing or improving visualisations, usually in the form of dashboards. Yet another future work that was mentioned by several papers was to analyse non-textual data together with the textual data. This could be in the form of numerical data, such as exam grades or Likert-scale responses, or it could be video or audio recordings. We can also add improving performance of the methods and extending the current system with more features. Exactly what is referred to here depends very much on the AI method or system developed and tested in the study. Examples can be to create a more domain specific sentiment lexicon, combine individual methods into a a bigger system, extend with multilingual support and reduce dimensionality and resource requirements.

\subsection{Current status}
As mentioned, the work on the SLR is not completed and will resume in the spring semester 2025, with the aim of publishing it eventually on the side of the master thesis. At this point in time, we have completed the study selection, and we have carried out data extraction for most of the 58 included papers. The data extraction still requires some work, however. We will in the following attempt to explain the next steps for the SLR.

\begin{enumerate}
    \item \textit{Quality assessment}: The quality of the included papers must be assessed. For this we must develop some quality assessment rules to be applied on all 58 papers. This will ensure that the papers contain sufficient information for our needs and that the results of the studies are truly relevant for our SLR.
    \item \textit{Research questions}: While we have an outline for what data shall be reported in the SLR and an idea of what the SLR should accomplish, we still need to define concrete research questions that can be answered during sythesis of the results.
    \item \textit{Continue data extraction}: We have already extracted data from 48 or the 
    58 included papers, but the codes need to be refined further to make the reporting of the data more meaningful and valuable. The remaining papers need to have data extracted and the formatting of the Excel document must be formatted to make data analysis easier.
    \item \textit{Data synthesis}: After reporting the extracted data, we must make sense of the findings and use the insight to answer our research questions.
    \item \textit{Write the paper}: When we have answered our research questions, the paper itself must be written. This will include explaining the rationale of our SLR, present our research questions and why they are relevant, present the employed SLR methodology, report the results of the paper selection process, report the data from the included papers, answer the research questions, and conclude on what we have achieved with the SLR.
\end{enumerate}

Until now, we have been three people working on this SLR. This includes the two of us - in the form of two master students - and a PhD candidate. In the coming semester, the work will mainly be conducted by the PhD candidate while we focus on the master thesis. To perform our experiments on the collected open-text data next semester, we will be dependent on using insights from the SLR as a guide in choosing which AI methods to test, which preprocessing steps and tools may be effective and how different AI methods can be combined to provide more actionable insights from the open-text data.

\section{Proprietary solutions}
In addition to consulting literature, we have looked into how the two response systems, Mentimeter and Kahoot!, use AI to analyse open-text responses.

Mentimeter provides two options for analysing open-text responses using AI. The summary feature can be used for the open-ended and word-cloud question types. After a few seconds of processing, Mentimeter will provide a summary in the form of a handful of key points, each around five words with a suitable emoji. The summary feature is briefly explained in \cite{mentisummary}. The grouping feature, explained in \cite{mentigrouping}, can be used for the open-ended question type. If a question has more than ten responses, they can be grouped according to themes. Labels are automatically assigned to each group, and the user can select a group to see the related responses. Except for explaining these features themselves, Mentimeter does not provide any information on how they are implemented, however, from what we have found.

Kahoot! uses AI for clustering ideas submitted in response to their brainstorming question type. One of the developers at Kahoot!, who took part in implemented the clustering algorithm, explains how it works in \cite{kahootclustering}. He explains here that they have chosen an unsupervised clustering method. It uses the \textit{xlm-r-bert-base-nli-stsb-mean-tokens} model from the \textit{Sentence Transformers} Python library to create embeddings for the open-text responses. These embeddings are vector representations optimised for comparing semantic similarity between pieces of text. They then use a classical clustering method - he mentions \textit{K-means} in the article - and follow the \textit{elbow method} with a few optimisations to choose the appropriate number of clusters. As they had performance problems due to high dimentionality, they applied the \textit{TruncatedSVD} module from the \textit{scikit-learn} library to reduce it. While this algorithm has worked well, the article, which is from 2022, underlines that updates are in order. Newer transformer models have been made available and should be tested, auto-encoders should be explored to help reduce dimentionality of the embeddings, and other clustering algorithms should be checked out, such as \textit{HDBSCAN} and \textit{FAISS}.

All of these features are tried and tested in practice, which goes to show that real-time analysis of open-text responses through response systems is, indeed, viable. While Mentimeter and Kahoot! have implemented clustering and summarisation, we intend to look into more methods, such as sentiment analysis, as well as whether methods can be combined.

\section{Open problems in the domain}
\todo{What future research is needed in the field?}




% Keep preliminary RQ as a goal instead of finding a concrete RQ

% \section{Preliminary research question for master thesis}
% \todo{Create a research question for next semester based on the findings}

% Suggestion: How to get actionable insights from students' open-text responses through the use of AI?